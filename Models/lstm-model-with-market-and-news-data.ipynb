{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "225708f447eee93041881f9d6c3a3e890cb16718"
   },
   "source": [
    "## A simple LSTM model\n",
    "\n",
    "I combined techinques and code from two notebooks that I found, and converted it to an LSTM. \n",
    "\n",
    "References to other notebooks used: \n",
    "\n",
    "https://www.kaggle.com/christofhenkel/market-data-nn-baseline\n",
    "\n",
    "https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from kaggle.competitions import twosigmanews\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
   },
   "outputs": [],
   "source": [
    "# get the data from two sigma environment\n",
    "env = twosigmanews.make_env()\n",
    "(market_train_df, news_train_df) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8174e0fe351b1c2c667fb463406975e941aba9c"
   },
   "outputs": [],
   "source": [
    "# gets used later to aggregate news into marker data \n",
    "news_cols_agg = {\n",
    "    'bodySize': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n",
    "}\n",
    "# specify categorical columns\n",
    "categorical_cols = ['assetName', 'dayofweek', 'month', 'year']\n",
    "# lengths of embeddings of categorical columns\n",
    "embedding_lengths = [100, 2, 2, 3]\n",
    "encodings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c5a87dde9b7931382da32d0b362116fb8fbc94c"
   },
   "outputs": [],
   "source": [
    "def join_market_news(market_train_df, news_train_df):\n",
    "    # Fix asset codes (str -> list)\n",
    "    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n",
    "    \n",
    "    # Expand assetCodes -- converts ['AAPL', 'GOOG'] --> 'APPL', 'GOOG'\n",
    "    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n",
    "    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n",
    "\n",
    "    assert len(assetCodes_index) == len(assetCodes_expanded)\n",
    "    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n",
    "\n",
    "    # Create expandaded news (will repeat every assetCodes' row)\n",
    "    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n",
    "    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n",
    "\n",
    "    # Free memory\n",
    "    del news_train_df, df_assetCodes\n",
    "\n",
    "    # Aggregate numerical news features\n",
    "    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n",
    "    \n",
    "    # Free memory\n",
    "    del news_train_df_expanded\n",
    "\n",
    "    # Convert to float32 to save memory\n",
    "    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n",
    "\n",
    "    # Flat columns\n",
    "    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n",
    "\n",
    "    # Join with train\n",
    "    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n",
    "\n",
    "    # Free memory\n",
    "    del news_train_df_aggregated\n",
    "    \n",
    "    return market_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "969511254abbe99bfbb38c4e85422596635cbd30"
   },
   "outputs": [],
   "source": [
    "def get_xy(market_train_df, news_train_df):\n",
    "    x = get_x(market_train_df, news_train_df)\n",
    "    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n",
    "    return x, y\n",
    "\n",
    "def label_encode(series, min_counts=2):\n",
    "    vc = series.value_counts()\n",
    "    #reserve 0 for unknown\n",
    "    le = {c : i+1 for i, c in enumerate(vc.index[vc > min_counts])}\n",
    "    le['UNKN'] = 0\n",
    "    return le\n",
    "\n",
    "def get_encodings(df, cat_cols):\n",
    "    if len(encodings) == 0:\n",
    "        for col in cat_cols:\n",
    "            encodings[col] = label_encode(df[col])\n",
    "    return encodings\n",
    "\n",
    "def map_encodings(df, cat_cols, encs):\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].map(encs[col]).fillna(0).astype(int)\n",
    "        \n",
    "def get_x(market_train_df, news_train_df, isTrain=True):\n",
    "    # Split date into before and after 22h (the time used in train data)\n",
    "    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n",
    "    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)    \n",
    "    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n",
    "\n",
    "    # Round time of market_train_df to 0h of curret day\n",
    "    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n",
    "\n",
    "    # Join market and news\n",
    "    x = join_market_news(market_train_df, news_train_df)\n",
    "    \n",
    "    x['dayofweek'], x['day'], x['month'], x['year'] = x.time.dt.dayofweek, x.time.dt.day, x.time.dt.month, x.time.dt.year\n",
    "\n",
    "    encodings = get_encodings(x, categorical_cols)\n",
    "    map_encodings(x, categorical_cols, encodings) \n",
    "    if isTrain:\n",
    "        cols_to_drop = ['returnsOpenNextMktres10', 'universe', 'time']\n",
    "    else: \n",
    "        cols_to_drop = ['time']\n",
    "    \n",
    "    x.drop(columns=cols_to_drop, inplace=True)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31cd42303a383d2a6537c6f7b99ec2d298f26ff7"
   },
   "outputs": [],
   "source": [
    "# This will take some time...\n",
    "X, y = get_xy(market_train_df, news_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "068cd43c59e905593126201737f8a82d1024cdfb"
   },
   "outputs": [],
   "source": [
    "#Save universe data for latter use\n",
    "universe = market_train_df['universe']\n",
    "time = market_train_df['time']\n",
    "\n",
    "# Free memory\n",
    "del market_train_df, news_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f3fdd10e5debe4d487c0b52b944df75bf13e62f"
   },
   "outputs": [],
   "source": [
    "#get all the numeric columns\n",
    "num_cols = [x for x in X.columns if x not in categorical_cols]\n",
    "\n",
    "#remove assetCode from num_cols\n",
    "num_cols = [x for x in num_cols if x not in ['assetCode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94cec422fc877d8aba81c797d34b06d2fdff21b1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scale numeric cols\n",
    "def scale_numeric(df):\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    #need to do this due to memory contraints\n",
    "    for i in range(0, len(num_cols), 4):\n",
    "        cols = num_cols[i:i + 3]\n",
    "        df[cols] = scaler.fit_transform(df[cols].astype(float))\n",
    "        \n",
    "scale_numeric(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "676b28bbf482d680e642c6c7450bf5c918399467"
   },
   "outputs": [],
   "source": [
    "#split dataset into 80% for training and 20% for validation \n",
    "n_train = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\n",
    "X_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a172258a46c7db96e85964d6d56c24f0ec56f0c"
   },
   "outputs": [],
   "source": [
    "# For valid data, keep only those with universe > 0. This will help calculate the metric\n",
    "u_valid = (universe.iloc[n_train:] > 0)\n",
    "t_valid = time.iloc[n_train:]\n",
    "\n",
    "X_valid = X_valid[u_valid]\n",
    "y_valid = y_valid[u_valid]\n",
    "t_valid = t_valid[u_valid]\n",
    "\n",
    "d_valid = t_valid.dt.date\n",
    "\n",
    "del u_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a8d1bb2d2d2a4b4a9202912578a0255bace89c2"
   },
   "outputs": [],
   "source": [
    "#seperate the columns into categorical and numerical\n",
    "def get_cat_num_split(df):\n",
    "    X = {} \n",
    "    X['num'] = df.loc[:, num_cols].values\n",
    "    X['num'] = np.reshape(X['num'], (X['num'].shape[0], 1, X['num'].shape[1]))\n",
    "    for cat in categorical_cols:\n",
    "        X[cat] = df.loc[:, cat].values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ba3082b4c57fdc6016571cb4688f262512aa6ff"
   },
   "outputs": [],
   "source": [
    "#seperate the columns into categorical and numerical\n",
    "X_train_split = get_cat_num_split(X_train)\n",
    "X_valid_split = get_cat_num_split(X_valid)\n",
    "\n",
    "#set y to a binary representation of returns, true if it's 0-1 and false if i'ts -1-0\n",
    "y_train_bin = (y_train >= 0).values\n",
    "y_valid_bin = (y_valid >= 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93f4e47393778cbe5040e3aee5fc6fdc5505c41e"
   },
   "outputs": [],
   "source": [
    "encoding_len = {k: len(encodings[k]) + 1 for k in encodings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df9c3f37a0344522e9f8ed3253242d3c00ac9114"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Concatenate, Flatten, LSTM, Dropout, Reshape\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "cat_inputs = [Input(shape=[1], name=cat) for cat in categorical_cols]\n",
    "embeddings = [Embedding(encoding_len[cat], embedding_lengths[i])(cat_inputs[i]) for i, cat in enumerate(categorical_cols)]\n",
    "categorical_logits = Concatenate()([(cat_emb) for cat_emb in embeddings])\n",
    "categorical_logits = LSTM(128, activation='relu', input_shape=(1, len(categorical_cols)), return_sequences=True,\n",
    "                         kernel_regularizer=l2(1e-5), kernel_initializer='random_uniform')(categorical_logits)\n",
    "\n",
    "numerical_inputs = Input(shape=(1, len(num_cols)), name='num')\n",
    "numerical_logits = LSTM(256, activation='relu', input_shape=(1, len(num_cols)), return_sequences=True,\n",
    "                        kernel_regularizer=l2(1e-5), kernel_initializer='random_uniform')(numerical_inputs)\n",
    "numerical_logits = Dropout(DROPOUT_RATE)(numerical_logits)\n",
    "\n",
    "logits = Concatenate()([numerical_logits,categorical_logits])\n",
    "logits = LSTM(256, activation='relu', kernel_initializer='random_uniform')(logits)\n",
    "out = Dense(1, activation='sigmoid', name='confidence_level')(logits)\n",
    "\n",
    "model = Model(inputs = cat_inputs + [numerical_inputs], outputs=out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ea6bee7b105d40125e546097fb5cb9eabe56e87"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d221c6488b13ef0b5be8c54fc1ccc6b11bbe858"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "check_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(patience=1,verbose=True)\n",
    "model.fit(X_train_split, y_train_bin,\n",
    "          validation_data=(X_valid_split, y_valid_bin),\n",
    "          epochs=1,\n",
    "          verbose=True,\n",
    "          callbacks=[early_stop,check_point]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73a12439b5d43ee8d054d8b7b32c3de09b757543"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# distribution of confidence that will be used as submission\n",
    "model.load_weights('model.hdf5')\n",
    "# scaled condifence value  from 0 - 1 to -1 - 1\n",
    "confidence_valid = model.predict(X_valid_split)[:,0]*2 -1\n",
    "\n",
    "plt.hist(confidence_valid, bins='auto')\n",
    "plt.title(\"predicted confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d49cc34a1c7e7a292744c93e4b6891290052561d"
   },
   "outputs": [],
   "source": [
    "#r_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\n",
    "x_t_i = confidence_valid * y_valid\n",
    "data = {'day' : d_valid, 'x_t_i' : x_t_i}\n",
    "df = pd.DataFrame(data)\n",
    "x_t = df.groupby('day').sum().values.flatten()\n",
    "mean = np.mean(x_t)\n",
    "std = np.std(x_t)\n",
    "score_valid = mean / std\n",
    "print(score_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79f77f74583568d57c5caec118ffa8e00687b1e3"
   },
   "source": [
    "# Train full model\n",
    "Now we train a full model with `num_boost_round` found in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "3f8925ddb4c42201fdba38efac258ca38773849d"
   },
   "outputs": [],
   "source": [
    "def make_predictions(predictions_template_df, market_obs_df, news_obs_df):\n",
    "    inp = get_x(market_obs_df, news_obs_df, False)\n",
    "    scale_numeric(inp)\n",
    "    inp_split = get_cat_num_split(inp)\n",
    "    scaled_pred = model.predict(inp_split) * 2 - 1    \n",
    "    predictions_template_df.confidenceValue = np.clip(scaled_pred, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "724c38149860c8e9058474ac9045c2301e8a20da",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()\n",
    "\n",
    "x1,y1,z1 = None, None, None\n",
    "\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    x1,y1,z1 = predictions_template_df, market_obs_df, news_obs_df\n",
    "    make_predictions(x1,y1,z1)\n",
    "    env.predict(predictions_template_df)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
   },
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
